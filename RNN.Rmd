---
Tutorial Title: "Recurrent Neural Networks"
Authors: "Tan Peck Kee", "Max Koh Junran"
Submission Date:
Course Name and Number: 'Statistical and Machine Learning 40.319'
Semester/Year: "2022 January Term"

output: 
  prettydoc::html_pretty:
    theme: cayman
    highlight: github
    toc: true
    toc_depth: 3
    number_sections: true
---

# Table of Contents

0. Motivation: Hook
1. Context: Prerequisite ideas
2. Theory: RNN theory
3. How To: RNN for IMDB Sentiment Analysis
4. How To: RNN for Weather Prediction
5. Comparing CNN and RNN for time series prediction
6. Sources

# 0. Motivation
We want to introduce the concept or Recurrent Neural Networks (RNN) because of its ability to handle sequential data. An analysis that could be done with RNN is Sentiment Analysis. Sentiment Analysis allows companies to understand which of their products are well-liked or disliked through the count of positive and negative sentiments. An example would be IMDB movie reviews sentiment analysis, to understand which movies are well-liked or disliked by customers.


# 1. Prerequisite ideas
So before we dive in to learning about Recurrent Neural Networks (RNN), there are two ideas we need to keep in mind. These ideas should be at least somewhat familiar to you, so we will go through them briefly. 

These two ideas are:
1. Recurrence 
2. Neural Networks 


# 1.1 Recurrence
In computer science, recurrence can be thought of as defining a problem as an "earlier" version of itself. A simple example would be how to calculate factorials. The recursive formula for doing so can be formulated as such:

$$
n! = n \times (n-1)! \\
(n-1)! = (n-1) \times (n-2)! \\ 
(n-2)! = (n-2) \times (n-3)!\\
.\\
.\\
$$
As can be seen, the calculation of $n!$ is dependent on this value of $ (n-1)! $, which is a previous "state" of the factorial, and in turn $(n-1)!$ is dependent on $(n-2)!$, all the way down to the first term. Keep in mind this idea of being dependent on the previous state.

Another way to understand this idea would be the concept of Exponential smoothing

Exponential smoothing is a forecasting technique whereby the prediction in the next period is dependent on the actual and predicted value of the previous period.

$$
F_{t+1} = \alpha A_t + (1- \alpha) F_t
$$
where $F_{t+1}$ is the forecast for the next period, and $F_t$ and $A_t$ are the forecast and actual value for the current period. $\alpha$ is simply the learning parameter influencing how much the forecasted value of the following period is affected by the forecasted value and actual value of the current period. Naturally, the forecast for the current period can then be formulated as a function of the previous period:
$$
F_{t} = \alpha A_{t-1} + (1- \alpha) F_{t-1}
$$
Finally, in general we can formulate $F_t$ as:

$$
F_{t} = \alpha A_{t-1} + (1- \alpha) F_{t-1} \\ 
= \alpha A_{t-1} + (1- \alpha) (\alpha A_{t-2} + (1- \alpha) F_{t-2}) \\
.\\
.\\
.\\
= \alpha (A_{t-1}+(1-\alpha)^1A_{t-2}+(1-\alpha)^2A_{t-3}+...+(1-\alpha)^{t-1}A_{1}) +(1-\alpha)^tF_{0}\\
= f(A_{t-1},A_{t-2},...,A_{2},A_{1})
$$
As you can see, once we expand the formulation, the forecasted value for period $t$ is dependent on all values of the previous states.

Bringing back the idea of recurrence being a formulation of a problem based on its previous states, we can see how exponential smoothing is a familiar example (hopefully) which you can use to understand what recurrence is. 


# 1.2 Neural networks

Now lets look at the second idea we need to be familiar with: Neural networks

As before, I'm assuming you have at least some familiarity with Neural Networks, so I will just be highlighting some key concepts that will be important to keep in mind.

Firstly, lets revise the basic structure of a neural network:

(picture of neural network,p1)

A neural network will consists of nodes and layers, each layer will have an activation function to pass their output to the next layer. This connections between the nodes and layers connects the input variables to the output variables.

Input data $x$ is fed into the network to make a prediction on $\hat{y}$. The difference between the predicted $\hat{y}$ and the actual $y$ is then used to adjust the weights of the network such that the difference between the predicted and actual (otherwise called the loss) is minimized. This is how the network "learns". That said, this idea of minimizing loss is not unique to neural networks, it can be found on other machine learning algorithms such as Logistic regression or KNN.

So for us, an important concept that is key to understand that is also unique to Neural networks is the idea of Forward and Backward propagation. In summary, forward propagation is how the network calculates the predicted value and hence the loss, and the backward propagation calculates the partial derivatives of the loss function and hence the adjusted weights. (via gradient descent)

Lets look at how its done:

Here is the detailed example network we will use to demonstrate the calculation
(p1.1)

Forward propagation
Using the example network, its forward calculations will be:
$$
h = f(\textbf{w}^Tx+b^T)\\
y = \phi(\boldsymbol{\omega}h+\beta)
$$
(note that $h$ is a 3x1 column vector, and the weights and bias $w,\omega,b$ are row vectors 1x3)

Backward propagation
For the same example network, the backward calculations will be:
First lets define the loss function:
$$
L(\hat{y},y)=\frac1n \sum (\hat{y}-y)^2 \\
= \frac1n \sum (\phi(\boldsymbol{\omega}^Tf(\textbf{w}^Tx+b)+\beta)-y)^2
$$
Then the partial derivatives.
$$
\frac{\delta L}{\delta \beta} = \frac2n \sum (\phi(\boldsymbol{\omega}^Tf(\textbf{w}^Tx_i+b)+\beta)-y) \phi'(\boldsymbol{\omega}^Tf(\textbf{w}^Tx_i+b)+\beta) \\
\frac{\delta L}{\delta \omega} = \frac2n \sum (\phi(\boldsymbol{\omega}^Tf(\textbf{w}^Tx_i+b)+\beta)-y) \phi'(\boldsymbol{\omega}^Tf(\textbf{w}^Tx_i+b)+\beta)f(\textbf{w}^Tx_i+b) \\
\frac{\delta L}{\delta b} = \frac2n \sum (\phi(\boldsymbol{\omega}^Tf(\textbf{w}^Tx_i+b)+\beta)-y) \phi'(\boldsymbol{\omega}^Tf(\textbf{w}^Tx_i+b)+\beta)diag(w)f'(\textbf{w}^Tx_i+b) \\
\frac{\delta L}{\delta b} = \frac2n \sum (\phi(\boldsymbol{\omega}^Tf(\textbf{w}^Tx_i+b)+\beta)-y) \phi'(\boldsymbol{\omega}^Tf(\textbf{w}^Tx_i+b)+\beta)x_idiag(w)f'(\textbf{w}^Tx_i+b) \\
$$

Keep in mind this idea of the logic behind forward and backward propagation, as well as the structure of a neural network (this will be useful).


# 2. Recurrent Neural Networks

# 2.1 Introduction to RNN
Now that we have revised the prerequisite concepts of recurrence and neural networks, its time to fuse them together in a beautiful harmony to produce a Recurrent Neural Network. (how exciting!)

The goal of a Recurrent neural network is to process sequential information; that is, information where the next data point is dependent on the previous data point. Examples of Sequential information includes: Words/Sentences, Time series data, etc.

To make a comparison to the basic Neural Network that we looked at, the basic Neural network is not able to capture or "understand" the information in a sequence, only as distinct variables (that could possibly have dependency to one another), but not in a sequence.

Here is an example of an application of RNN:

(picture of google search predicting next word,p2)

As mentioned, words/sentences are an example of sequential data, and RNNs are adept at processing such information to make predictions about the next data point (in this case next word) in the sequence.

Generally, we can formulate sequential data as follows:

$$
\textbf{x}_i = (x_i(t),x_i(t-1),...,x_i(2),x_i(1)) \\

\textbf{x} = \begin{pmatrix}
x_1 \\
x_2 \\
\vdots \\
x_n \\
\end{pmatrix}
$$
# 2.2 Architecture of RNN

So what does the architecture of the RNN look like? Essentially it has all the elements of a basic neural network, in addition to a recurrent layer.

Here is an illustration of a simple RNN.

(picture of simple RNN,p3)

As you can see, just like the normal neural network, it has input and output layers. The main difference then is the presence of a recurrent layer.

Lets zoom in on the recurrent layer to better understand it.

(picture of recurrent layer,p4)

Essentially, the output of the recurrent layer is not only dependent on the input at the current time step $t$, but also dependent on the output of the previous output of at time $t-1$, and following that, the output of $t-1$ is dependent on $t-2$, and so on, bring in the idea of recurrence (which we have previously discussed), that the current formulation depends on all the previous formulation.

In mathematical terms,

$$
h_i(t)=f^t(x_i(t),x_i(t-1),...,x_i(2),x_i(1))
$$
where $f$ is the activation function of the layer and $x(t)$ is the sequential data, and the function $f$ is applied $t$ times. Notice how this looks similar to the expression in the example for exponential smoothing, which means then we can formulate this into a simpler recursive expression:

$$
h_i(t) = f(x_i(t),h_i(t-1))
$$
# 2.3 Forward and backward propogation

Lets use a more concrete example to now explain forward and backward propagation:

(picture of RNN,p5)

where $x(t),y(t),h(t)$ are the input, output, hidden output at time $t$, $u,w,\omega$ are the weights connecting the layers, $b$ is the bias of the hidden layer, $\beta$ is the bias of the output layer. The activation function $f$ at $h$ is ReLu, and output activation $\phi$ at $y$ is Linear. Notice the difference between this example and the previous basic neural network example is the inclusion of the weights $u$ attached to the output from a previous time step. 

Then, the forward calculations (which hopefully you are familiar with from the basic NN) can be written as such:

$$
h_i(t) = f(\textbf{w}^Tx_i(t)+diag(h_i(t-1))\textbf{u}^T+b^T)\\
y_i(t) = \phi(\boldsymbol{\omega}h_i(t)+\beta)
$$
(note that $h_i(.)$ is a 3x1 column vector, and the weights and bias $u,w,\omega,b$ are row vectors 1x3)

Notice the presence of a recurrent term when calculating the output of the hidden layer. This means that in order to know the output at a time step $t$, you need to have calculated the output of all previous time steps, starting from 0.

And for backward calculations, first lets define the loss function:

$$
L(\hat{y},y)=\frac1n \sum (\hat{y}_i-y_i)^2 \\
= \frac1n \sum (\phi(\boldsymbol{\omega}^Tf(\textbf{w}^Tx_i(t)+diag(h_i(t-1))\textbf{u}^T+\beta)-y_i)^2
$$
Then the backward calculations:
$$
\frac{\delta L}{\delta \beta} = \frac2n \sum (\phi(\boldsymbol{\omega}^Tf(\textbf{w}^Tx_i(t)+diag(h_i(t-1))\textbf{u}^T+b)+\beta)-y) \phi'(\boldsymbol{\omega}^Tf(\textbf{w}^Tx_i(t)+diag(h_i(t-1))\textbf{u}^T+b)+\beta) \\
\frac{\delta L}{\delta \omega} = \frac2n \sum (\phi(\boldsymbol{\omega}^Tf(\textbf{w}^Tx_i(t)+diag(h_i(t-1))\textbf{u}^T+b)+\beta)-y) \phi'(\boldsymbol{\omega}^Tf(\textbf{w}^Tx_i(t)+diag(h_i(t-1))\textbf{u}^T+b)+\beta)f(\textbf{w}^Tx_i(t)+diag(h_i(t-1))\textbf{u}^T+b) \\
\frac{\delta L}{\delta b} = \frac2n \sum (\phi(\boldsymbol{\omega}^Tf(\textbf{w}^Tx_i(t)+diag(h_i(t-1))\textbf{u}^T+b)+\beta)-y) \phi'(\boldsymbol{\omega}^Tf(\textbf{w}^Tx_i(t)+diag(h_i(t-1))\textbf{u}^T+b)+\beta)diag(w)f'(\textbf{w}^Tx_i(t)+diag(h_i(t-1))\textbf{u}^T+b) \\
\frac{\delta L}{\delta b} = \frac2n \sum (\phi(\boldsymbol{\omega}^Tf(\textbf{w}^Tx_i(t)+diag(h_i(t-1))\textbf{u}^T+b)+\beta)-y) \phi'(\boldsymbol{\omega}^Tf(\textbf{w}^Tx_i(t)+diag(h_i(t-1))\textbf{u}^T+b)+\beta)x_i(t)diag(w)f'(\textbf{w}^Tx_i(t)+diag(h_i(t-1))\textbf{u}^T+b) \\
\frac{\delta L}{\delta u} = \frac2n \sum (\phi(\boldsymbol{\omega}^Tf(\textbf{w}^Tx_i(t)+diag(h_i(t-1))\textbf{u}^T+b)+\beta)-y) \phi'(\boldsymbol{\omega}^Tf(\textbf{w}^Tx_i(t)+diag(h_i(t-1))\textbf{u}^T+b)+\beta)diag(h_i(t-1))diag(w)f'(\textbf{w}^Tx_i(t)+diag(h_i(t-1))\textbf{u}^T+b) \\
$$
Once again notice the presence of the recurrent term in the partial derivative of $\frac{\delta L}{\delta u}$, meaning that to calculate the loss and gradient of the current time step, you will need to have done the calculations in all previous time steps.

# 2.4 Summary of RNN

In conclusion, the main feature of a recurrent neural network is the presence of the recurrent architecture in the network, which takes retains the memory of previous calculations to make a calculation in the current time step. That is, for a sequential data $\textbf{x} = (x(t),x(t-1),...,x(2),x(1))$, the output $y(t)$ is affected by all of $\textbf{x}$, compared to how a basic neural network does its calculation, where the output $y_i$ is only affected by $x_i$. (so long as weights and bias do not change.)

To add on, there are a few further considerations/limitations to keep in mind for an RNN.

Firstly, because the data is sequential and the calculations (which we have went through) require to have done calculation of previous instances/time steps, an RNN is not compatible with parallel processing, meaning it might take longer than a similarly complex network to train.

Secondly, the basic RNN suffers from the limitation of having "short-term memory". This is due to the possibility of gradients exploding or vanishing especially in a long sequence, causing the network to "lose memory" of previous states earlier in the sequence. That said, there are solutions to deal with this problem, like LSTM or Attention, which are more complex topics so we will have to cover that in a separate tutorial.


# 3. How To: RNN for IMDB Sentiment Analysis

Now that we have looked at the theory behind RNN, lets move on to implementing it

# Install Keras
```{r}
# devtools::install_github("rstudio/keras")
library(keras)
```

# Data Preprocessing
Machine Learning Algorithms could not deal with raw text, have to convert the text into numbers before feeding it into an algorithm. The dataset we will be using for this tutorial have done this step. Changing the raw text of IMDB sentiments into numbers with a process called: **Tokenization**.
```{r}
#consider only the top 2,500 unique words in the dataset
max_unique_word <- 2500

# Load the dataset
my_imdb <- dataset_imdb(num_words= max_unique_word)
```


# Training and Testing Datasets
Splitting into training and testing datasets, with the training set to train our RNN model and the testing dataset to evaluate our model's accuracy.

The input (x) are: Movie reviews from IMDB.They have been pre-processed into numerical vectors through a process called **Tokenization**.

These input (x) also have go through a process called **Padding** or **Truncating**. This is to ensure that each movie review being fed into the model is of equal length. Since some movie reviews might be longer than the number of words we set (100), we have to truncate it or it might be shorter than the number of words we set(100), we have to pad it. 

The labels (y) are: "1" for Positive and "0" for Negative sentiment.
```{r}
x_train <- my_imdb$train$x
y_train <- my_imdb$train$y
x_test <- my_imdb$test$x
y_test <- my_imdb$test$y

#Padding and Truncating Process

#cut off reviews after 100 words
max_review_len <- 100

x_train <- pad_sequences(x_train, maxlen=max_review_len)
x_test <- pad_sequences(x_test, maxlen=max_review_len)
```

```{r}
# An Example of a Sentiment
word_to_index <- dataset_imdb_word_index()
index_to_word <- names(word_to_index)
names(index_to_word) <- word_to_index


decoded_review <- sapply(x_train[1,], function(index) {
  word <- if (index >= 3) index_to_word[[as.character(index - 3)]]
  if (!is.null(word)) word else "?"
})



first_sentiment_result <- y_train[1]

print("An example of a sentiment would be: ")
cat(decoded_review)
print("================")
print("with the sentiment of: ")
if(first_sentiment_result==1)print("Positive Sentiment") else print("Negative Sentiment")
```

# Simple Recurrent Neural Network (RNN)
The changes made to what we have learnt from Artificial Neural Network (ANN) lecture:

**layer_embedding()** :This is used to fit an embedding layer based on the training dataset. Embedding is needed to give the categorical integers (from tokenization) to more meaningful representations. The word embedding captures the inherited relationship of words and dramatically reduces the input dimension (128 dims). 

**layer_simple_rnn()** : This is used to add a simple RNN layer. This layer encapsulates what we have explained earlier namely Recurrence + Neural Networks. 

```{r}
#Build the RNN Model
rnn_model <- keras_model_sequential()
rnn_model %>% 
  layer_embedding(input_dim = max_unique_word, output_dim=128) %>% 
  layer_simple_rnn(units=64, dropout=0.2, recurrent_dropout=0.2) %>% 
  layer_dense(units=1, activation="sigmoid")
```


```{r}
#Compile the RNN Model
rnn_model %>% compile(
  loss="binary_crossentropy",
  optimizer="adam",
  metrics=c("accuracy")
)

```

```{r}
#Fitting the RNN Model
rnn_history <- rnn_model %>%  fit(
  x_train, y_train,
  batch_size= 128,
  epochs=5,
  validation_split=0.2,
  verbose=0
)
```
```{r}
#Evaluate the RNN Model
rnn_model %>% 
  evaluate(x_test, y_test)
```

# 4. How To: RNN for Weather Prediction

# Packages Required
```{r}
library(keras) 
library(deepviz)
library(tidyverse)
```

# Application: Weather Prediction
Use an RNN to predict whether it will rain in Seattle given information on whether it rained or not for the previous six days.

Dataset
- Information on the weather of Seattle for every single day between 1 January 1948 to 12 Decemeber 2017.
- DATE: The date of the observation.
- PRCP: Amount of Precipitation (in inches).
- TMAX: Maximum temperature for that day (in degrees Fahrenheit).
- TMIN: Minimum temperature for that day (in degrees Fahrenheit).
- RAIN: Whether did it rain that day (TRUE/ FALSE).
```{r}
#Read dataset
weather_data <- read.csv("seattleWeather_1948-2017.csv")

# View(weather_data)

head(weather_data)
```

# Data Wrangling
- Some pre-processing steps are needed to get our data ready to feed into our model. 
- Moving-Block Sub-Sampling 
- Ensuring the values in weather_matrix is numeric (as.integer)
- Ensuring there is no NaN's in weather_matrix
```{r}
# Rain column from weather_data
rain <- weather_data$RAIN

max_len <- 6 # Number of previous days we will be looking at

#Chop rain vector up into samples of (max_len +1)
# Since we want to know whether next day would rain or not, this is to see how good we did in predicting it.
round(length(rain)/ (max_len +1))
#3650 examples isn't enough to train a deep learning model. 

#To stretch out our data, used moving-block sub-sampling 
move_by <- 3

#First example will be n=1 to n=7, Second example will be n=4 to n=10, Third example will be n=7 to n=13 and so on.
example1 <- rain[1:7]
example2 <- rain[4:10]
example3 <- rain[7:13]

# Number of examples that could be obtained from using moving-block sub-sampling = 8515 examples
# 7 + (3*no_of_examples) = length(rain)
no_of_examples <- (length(rain)-7)/(move_by)
no_of_examples <- round(no_of_examples)

#Create an empty matrix to store the RAIN data with each row being 1 example
weather_matrix <- matrix(nrow=no_of_examples, ncol=(max_len +1))

# Fill up the weather_matrix with the 8515 examples
# as.integer ensures the values are numeric
for (i in 0:(no_of_examples-1) ){
  current_example <- as.integer(rain[(1+(i*3)):(7+(i*3))])
  weather_matrix[i+1,] <- current_example
}


```
```{r}
#Remove any NA values from weather_matrix
weather_matrix <- na.omit(weather_matrix)

#Check whether is there any NA values in weather_matrix
anyNA(weather_matrix)
```

# Data Preparation
Inputs, X: The previous 6 days.
Output, Y: The 7th day we are interested in predicting.
```{r}
X <- weather_matrix[,c(1:6)]
Y <- weather_matrix[,7]
```

# Train-Test Split
Training set: 80%
Testing set:  20%
Also, for X, its dimension should be 3 dimension.
- First dimension: Number of our training examples.
- Second dimension: Length of our input sequence = max_len.
- Third dimension: Number of features we have.

```{r}
rowcount <- nrow(weather_matrix)

set.seed(123)

trainrows <- sort(sample(rowcount, rowcount*0.8))

X_train <- array(X[trainrows, ], dim=c(length(trainrows), max_len, 1))
Y_train <- Y[trainrows]

X_test <- array(X[-trainrows, ], dim=c(rowcount-length(trainrows), max_len, 1))
Y_test <- Y[-trainrows]

#Check train=80% and test=20%
nrow(X_train) / (nrow(X_train)+nrow(X_test))
nrow(X_test) / (nrow(X_train)+nrow(X_test))


length(Y_test)
```
# Recurrent Neural Network in Keras
# Build Model
```{r}
model <- keras_model_sequential()

model %>% layer_dense(input_shape=c(6,1), units=6)

model %>% layer_simple_rnn(units=6)

model %>% layer_dense(units=1, activation="sigmoid")

summary(model)

plot_model(model)

```
# Compile Model
```{r}
model %>% compile(
  loss="binary_crossentropy",
  optimizer="RMSprop",
  metrics=c("accuracy"))
```

# Fit the data into the Model
```{r}
history <- model %>% fit(
  x=X_train,
  y=Y_train,
  batch_size = 20,
  epochs=20,
  validation_split=0.2, 
  verbose=0
)

plot(history)
```

# Predict with the trained RNN
```{r}
y_predicted <- model %>% predict(X_test)
# y_predicted

#Threshold = 0.5
y_predicted <- as.integer((y_predicted > 0.5))

#Confusion Matrix
CM <- table(Y_test, y_predicted)
CM

#Test set Accuracy
accuracy <- sum(diag(CM))/ sum(CM)
accuracy
```
This model is able to predict with 70.6% accuracy whether it is going to rain or not, based on past 6 days of data. 

# 5. Comparing CNN and RNN for time series prediction
```{r}
model2  <-  keras_model_sequential() %>% 
            layer_conv_1d(filters=64, kernel_size=2, strides=1, 
                          padding='same', use_bias=TRUE, activation='relu',
                          input_shape=c(6, 1), name='c1d') %>% 
            layer_max_pooling_1d(pool_size=2) %>% 
            layer_flatten() %>% 
            layer_dense(units=50, activation='relu') %>% 
            layer_dense(units=1, activation="sigmoid")

model2 %>% compile(
  loss="binary_crossentropy",
  optimizer="RMSprop",
  metrics=c("accuracy"))

history2 <- fit(model2, X_train, Y_train, epochs=20, verbose=0)
```

# Predict with the trained CNN
```{r}
y_predicted_2 <- model2 %>% predict(X_test)
# y_predicted

#Threshold = 0.5
y_predicted_2 <- as.integer((y_predicted_2 > 0.5))

#Confusion Matrix
CM_2 <- table(Y_test, y_predicted_2)
CM_2

#Test set Accuracy
accuracy_2 <- sum(diag(CM_2))/ sum(CM_2)
accuracy_2
```
This model is able to predict with 68.7% accuracy whether it is going to rain or not, based on past 6 days of data. 


# 6. Sources 

Weather Prediction: 1. https://www.kaggle.com/rtatman/beginner-s-intro-to-rnn-s-in-r

IMDB Sentiment Analysis:
1. http://ai.stanford.edu/~amaas/data/sentiment/
2. https://scientistcafe.com/ids/r/ch12rnn#Overview_for_IMDB_Dataset
3. https://rstudio-pubs-static.s3.amazonaws.com/486604_46a03b6e83204318bb3d13dad87c10e2.html