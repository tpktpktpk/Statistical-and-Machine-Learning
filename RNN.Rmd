---
title: "Recurrent Neural Networks, Statistical and Machine Learning 40.319"
author: "Tan Peck Kee, Max Koh Junran"
date: "2022 January Term, 18 April 2022"
output: 
  prettydoc::html_pretty:
    theme: cayman
    highlight: github
    toc: true
    toc_depth: 3
    number_sections: true
---
# Header 
1. Tutorial title: Recurrent Neural Networks
2. Authors: Tan Peck Kee, Max Koh Junran
3. Submission date: 18 April 2022
4. Course Name and Number: Statistical and Machine Learning 40.319
5. Semester/Year: 2022 January Term

# Table of Contents
0. Motivation: Hook
1. Context: Prerequisite ideas
2. Theory: RNN theory
3. How To: RNN for IMDB Sentiment Analysis
4. How To: LSTM for IMDB Sentiment Analysis
5. Sources

# Packages Required 
For this analysis, we need **keras** library.
Uncomment **devtools::install_github("rstudio/keras")** if keras is not installed before. 
```{r}
# devtools::install_github("rstudio/keras")
library(keras)
```


# Motivation: Hook
With the vast number of movie reviews posted online, it is not possible for someone to manually analyze all of the movie reviews. Thus, we want to automate the analysis of movie reviews. For example, predicting the level of sentiment based on the review within a range (-1 , 1). 

A naive approach would be to check for occurrence of keywords, such as "great" being associated with positive review and "bad" being associated with negative review. One would recognize "This is a **great** movie!" to be a positive review. 

However, within a sentence, there is context! For example: "The popcorn was **great**!". With our naive approach, we would consider this movie review to be positive. However, as a human would read it, it is referring to the popcorn and not the movie. This established that Sentiment Analysis requires a method that captures context and sequences. Thus, we want to introduce the idea of Recurrent Neural Networks, which is known for handling sequential data for this task. 


# Context: Prerequisite ideas
So before we dive in to learning about Recurrent Neural Networks (RNN), there are two ideas we need to keep in mind. These ideas should be at least somewhat familiar to you, so we will go through them briefly. 

* These two ideas are:
  + Recurrence 
  + Neural Networks 


## Recurrence
In computer science, recurrence can be thought of as defining a problem as an "earlier" version of itself. A simple example would be how to calculate factorials. The recursive formula for doing so can be formulated as such:

$$
n! = n \times (n-1)! \\
(n-1)! = (n-1) \times (n-2)! \\ 
(n-2)! = (n-2) \times (n-3)!\\
.\\
.\\
$$
As can be seen, the calculation of $n!$ is dependent on this value of $ (n-1)! $, which is a previous "state" of the factorial, and in turn $(n-1)!$ is dependent on $(n-2)!$, all the way down to the first term. Keep in mind this idea of being dependent on the previous state.

Another way to understand this idea would be the concept of Exponential smoothing

Exponential smoothing is a forecasting technique whereby the prediction in the next period is dependent on the actual and predicted value of the previous period.

$$
F_{t+1} = \alpha A_t + (1- \alpha) F_t
$$
where $F_{t+1}$ is the forecast for the next period, and $F_t$ and $A_t$ are the forecast and actual value for the current period. $\alpha$ is simply the learning parameter influencing how much the forecasted value of the following period is affected by the forecasted value and actual value of the current period. Naturally, the forecast for the current period can then be formulated as a function of the previous period:
$$
F_{t} = \alpha A_{t-1} + (1- \alpha) F_{t-1}
$$
Finally, in general we can formulate $F_t$ as:

$$
F_{t} = \alpha A_{t-1} + (1- \alpha) F_{t-1} \\ 
= \alpha A_{t-1} + (1- \alpha) (\alpha A_{t-2} + (1- \alpha) F_{t-2}) \\
.\\
.\\
.\\
= \alpha (A_{t-1}+(1-\alpha)^1A_{t-2}+(1-\alpha)^2A_{t-3}+...+(1-\alpha)^{t-1}A_{1}) +(1-\alpha)^tF_{0}\\
= f(A_{t-1},A_{t-2},...,A_{2},A_{1})
$$
As you can see, once we expand the formulation, the forecasted value for period $t$ is dependent on all values of the previous states.

Bringing back the idea of recurrence being a formulation of a problem based on its previous states, we can see how exponential smoothing is a familiar example (hopefully) which you can use to understand what recurrence is. 


## Neural networks

Now lets look at the second idea we need to be familiar with: Neural networks

As before, I'm assuming you have at least some familiarity with Neural Networks, so I will just be highlighting some key concepts that will be important to keep in mind.

Firstly, lets revise the basic structure of a neural network:

![Neural Network](Images/Slide1.png)

A neural network will consists of nodes and layers, each layer will have an activation function to pass their output to the next layer. This connections between the nodes and layers connects the input variables to the output variables.

Input data $x$ is fed into the network to make a prediction on $\hat{y}$. The difference between the predicted $\hat{y}$ and the actual $y$ is then used to adjust the weights of the network such that the difference between the predicted and actual (otherwise called the loss) is minimized. This is how the network "learns". That said, this idea of minimizing loss is not unique to neural networks, it can be found on other machine learning algorithms such as Logistic regression or KNN.

So for us, an important concept that is key to understand that is also unique to Neural networks is the idea of Forward and Backward propagation. In summary, forward propagation is how the network calculates the predicted value and hence the loss, and the backward propagation calculates the partial derivatives of the loss function and hence the adjusted weights. (via gradient descent)

Lets look at how its done:

Here is the detailed example network we will use to demonstrate the calculation

![Neural Network](Images/Slide2.png)

Forward propagation
Using the example network, its forward calculations will be:
$$
h = f(\textbf{w}^Tx+b^T)\\
y = \phi(\boldsymbol{\omega}h+\beta)
$$
(note that $h$ is a 3x1 column vector, and the weights and bias $w,\omega,b$ are row vectors 1x3)

Backward propagation
For the same example network, the backward calculations will be:
First lets define the loss function:
$$
L(\hat{y},y)=\frac1n \sum (\hat{y}-y)^2 \\
= \frac1n \sum (\phi(\boldsymbol{\omega}^Tf(\textbf{w}^Tx+b)+\beta)-y)^2
$$
Then the partial derivatives.
$$
\frac{\delta L}{\delta \beta} = \frac2n \sum (\phi(\boldsymbol{\omega}^Tf(\textbf{w}^Tx_i+b)+\beta)-y) \phi'(\boldsymbol{\omega}^Tf(\textbf{w}^Tx_i+b)+\beta) \\
\frac{\delta L}{\delta \omega} = \frac2n \sum (\phi(\boldsymbol{\omega}^Tf(\textbf{w}^Tx_i+b)+\beta)-y) \phi'(\boldsymbol{\omega}^Tf(\textbf{w}^Tx_i+b)+\beta)f(\textbf{w}^Tx_i+b) \\
\frac{\delta L}{\delta b} = \frac2n \sum (\phi(\boldsymbol{\omega}^Tf(\textbf{w}^Tx_i+b)+\beta)-y) \phi'(\boldsymbol{\omega}^Tf(\textbf{w}^Tx_i+b)+\beta)diag(w)f'(\textbf{w}^Tx_i+b) \\
\frac{\delta L}{\delta b} = \frac2n \sum (\phi(\boldsymbol{\omega}^Tf(\textbf{w}^Tx_i+b)+\beta)-y) \phi'(\boldsymbol{\omega}^Tf(\textbf{w}^Tx_i+b)+\beta)x_idiag(w)f'(\textbf{w}^Tx_i+b) \\
$$

Keep in mind this idea of the logic behind forward and backward propagation, as well as the structure of a neural network (this will be useful).


# Theory: RNN theory

## Introduction to RNN
Now that we have revised the prerequisite concepts of recurrence and neural networks, its time to fuse them together in a beautiful harmony to produce a Recurrent Neural Network. (how exciting!)

The goal of a Recurrent neural network is to process sequential information; that is, information where the next data point is dependent on the previous data point. Examples of Sequential information includes: Words/Sentences, Time series data, etc.

To make a comparison to the basic Neural Network that we looked at, the basic Neural network is not able to capture or "understand" the information in a sequence, only as distinct variables (that could possibly have dependency to one another), but not in a sequence.

Here is an example of an application of RNN:

![Google Search](Images/Slide3.png)

As mentioned, words/sentences are an example of sequential data, and RNNs are adept at processing such information to make predictions about the next data point (in this case next word) in the sequence.

Generally, we can formulate sequential data as follows:

$$
\textbf{x}_i = (x_i(t),x_i(t-1),...,x_i(2),x_i(1)) \\
$$

$$
\textbf{x} = \begin{pmatrix}
x_1 \\
x_2 \\
\vdots \\
x_n \\
\end{pmatrix}
$$

## Architecture of RNN
So what does the architecture of the RNN look like? Essentially it has all the elements of a basic neural network, in addition to a recurrent layer.

Here is an illustration of a simple RNN.

![Simple Recurrent Neural Network](Images/Slide4.png)

As you can see, just like the normal neural network, it has input and output layers. The main difference then is the presence of a recurrent layer.

Lets zoom in on the recurrent layer to better understand it.

![Recurrent Neural Network](Images/Slide5.png)

Essentially, the output of the recurrent layer is not only dependent on the input at the current time step $t$, but also dependent on the output of the previous output of at time $t-1$, and following that, the output of $t-1$ is dependent on $t-2$, and so on, bring in the idea of recurrence (which we have previously discussed), that the current formulation depends on all the previous formulation.

In mathematical terms,

$$
h_i(t)=f^t(x_i(t),x_i(t-1),...,x_i(2),x_i(1))
$$
where $f$ is the activation function of the layer and $x(t)$ is the sequential data, and the function $f$ is applied $t$ times. Notice how this looks similar to the expression in the example for exponential smoothing, which means then we can formulate this into a simpler recursive expression:

$$
h_i(t) = f(x_i(t),h_i(t-1))
$$

## Forward and backward propogation

Lets use a more concrete example to now explain forward and backward propagation:

![Recurrent Neural Network](Images/Slide6.png)

where $x(t),y(t),h(t)$ are the input, output, hidden output at time $t$, $u,w,\omega$ are the weights connecting the layers, $b$ is the bias of the hidden layer, $\beta$ is the bias of the output layer. The activation function $f$ at $h$ is ReLu, and output activation $\phi$ at $y$ is Linear. Notice the difference between this example and the previous basic neural network example is the inclusion of the weights $u$ attached to the output from a previous time step. 

Then, the forward calculations (which hopefully you are familiar with from the basic NN) can be written as such:

$$
h_i(t) = f(\textbf{w}^Tx_i(t)+diag(h_i(t-1))\textbf{u}^T+b^T)\\
y_i(t) = \phi(\boldsymbol{\omega}h_i(t)+\beta)
$$
(note that $h_i(.)$ is a 3x1 column vector, and the weights and bias $u,w,\omega,b$ are row vectors 1x3)

Notice the presence of a recurrent term when calculating the output of the hidden layer. This means that in order to know the output at a time step $t$, you need to have calculated the output of all previous time steps, starting from 0.

And for backward calculations, first lets define the loss function:

$$
L(\hat{y},y)=\frac1n \sum (\hat{y}_i-y_i)^2 \\
= \frac1n \sum (\phi(\boldsymbol{\omega}^Tf(\textbf{w}^Tx_i(t)+diag(h_i(t-1))\textbf{u}^T+\beta)-y_i)^2
$$
Then the backward calculations:
$$
\frac{\delta L}{\delta \beta} = \frac2n \sum (\phi(\boldsymbol{\omega}^Tf(\textbf{w}^Tx_i(t)+diag(h_i(t-1))\textbf{u}^T+b)+\beta)-y) \phi'(\boldsymbol{\omega}^Tf(\textbf{w}^Tx_i(t)+diag(h_i(t-1))\textbf{u}^T+b)+\beta) \\
\frac{\delta L}{\delta \omega} = \frac2n \sum (\phi(\boldsymbol{\omega}^Tf(\textbf{w}^Tx_i(t)+diag(h_i(t-1))\textbf{u}^T+b)+\beta)-y) \phi'(\boldsymbol{\omega}^Tf(\textbf{w}^Tx_i(t)+diag(h_i(t-1))\textbf{u}^T+b)+\beta)f(\textbf{w}^Tx_i(t)+diag(h_i(t-1))\textbf{u}^T+b) \\
\frac{\delta L}{\delta b} = \frac2n \sum (\phi(\boldsymbol{\omega}^Tf(\textbf{w}^Tx_i(t)+diag(h_i(t-1))\textbf{u}^T+b)+\beta)-y) \phi'(\boldsymbol{\omega}^Tf(\textbf{w}^Tx_i(t)+diag(h_i(t-1))\textbf{u}^T+b)+\beta)diag(w)f'(\textbf{w}^Tx_i(t)+diag(h_i(t-1))\textbf{u}^T+b) \\
\frac{\delta L}{\delta b} = \frac2n \sum (\phi(\boldsymbol{\omega}^Tf(\textbf{w}^Tx_i(t)+diag(h_i(t-1))\textbf{u}^T+b)+\beta)-y) \phi'(\boldsymbol{\omega}^Tf(\textbf{w}^Tx_i(t)+diag(h_i(t-1))\textbf{u}^T+b)+\beta)x_i(t)diag(w)f'(\textbf{w}^Tx_i(t)+diag(h_i(t-1))\textbf{u}^T+b) \\
\frac{\delta L}{\delta u} = \frac2n \sum (\phi(\boldsymbol{\omega}^Tf(\textbf{w}^Tx_i(t)+diag(h_i(t-1))\textbf{u}^T+b)+\beta)-y) \phi'(\boldsymbol{\omega}^Tf(\textbf{w}^Tx_i(t)+diag(h_i(t-1))\textbf{u}^T+b)+\beta)diag(h_i(t-1))diag(w)f'(\textbf{w}^Tx_i(t)+diag(h_i(t-1))\textbf{u}^T+b) \\
$$
Once again notice the presence of the recurrent term in the partial derivative of $\frac{\delta L}{\delta u}$, meaning that to calculate the loss and gradient of the current time step, you will need to have done the calculations in all previous time steps.

## Summary of RNN

In conclusion, the main feature of a recurrent neural network is the presence of the recurrent architecture in the network, which takes retains the memory of previous calculations to make a calculation in the current time step. That is, for a sequential data $\textbf{x} = (x(t),x(t-1),...,x(2),x(1))$, the output $y(t)$ is affected by all of $\textbf{x}$, compared to how a basic neural network does its calculation, where the output $y_i$ is only affected by $x_i$. (so long as weights and bias do not change.)

To add on, there are a few further considerations/limitations to keep in mind for an RNN.

Firstly, because the data is sequential and the calculations (which we have went through) require to have done calculation of previous instances/time steps, an RNN is not compatible with parallel processing, meaning it might take longer than a similarly complex network to train.

Secondly, the basic RNN suffers from the limitation of having "short-term memory". This is due to the possibility of gradients exploding or vanishing especially in a long sequence, causing the network to "lose memory" of previous states earlier in the sequence. That said, there are solutions to deal with this problem, like LSTM or Attention, which are more complex topics so we will have to cover that in a separate tutorial.


# How To: RNN for IMDB Sentiment Analysis

Now that we have looked at the theory behind RNN, lets move on to implementing it.

Since we have motivated the problem of sentiment analysis, we will be analyzing movie reviews from IMDB, which houses numerous movie reviews online. 


## Install Keras
For this analysis, we need **keras** library.
```{r}
# devtools::install_github("rstudio/keras")
library(keras)
```

## Data Preprocessing
Machine Learning Algorithms could not deal with raw text. Therefore, we have to convert the text into numbers before feeding it into an algorithm. The dataset we will be using for this tutorial have done this step. Changing the raw text of IMDB sentiments into numbers with a process called: **Tokenization**.
```{r}
#consider only the top 2,500 unique words in the dataset
max_unique_word <- 2500

# Load the dataset
my_imdb <- dataset_imdb(num_words= max_unique_word)
```


## Training and Testing Datasets
Splitting into training and testing datasets, with the training set to train our RNN model and the testing dataset to evaluate our model's accuracy.

The input (x) are: Movie reviews from IMDB.They have been pre-processed into numerical vectors through a process called **Tokenization**.

These input (x) also have go through a process called **Padding** or **Truncating**. This is to ensure that each movie review being fed into the model is of equal length. Since some movie reviews might be longer than the number of words we set (100), we have to truncate it or it might be shorter than the number of words we set(100), we have to pad it. 

The labels (y) are: "1" for Positive and "0" for Negative sentiment.
```{r}
x_train <- my_imdb$train$x
y_train <- my_imdb$train$y
x_test <- my_imdb$test$x
y_test <- my_imdb$test$y

#Padding and Truncating Process

#cut off reviews after 100 words
max_review_len <- 100

x_train <- pad_sequences(x_train, maxlen=max_review_len)
x_test <- pad_sequences(x_test, maxlen=max_review_len)
```

## An example of a movie review
```{r}
# An Example of a review
word_to_index <- dataset_imdb_word_index()
index_to_word <- names(word_to_index)
names(index_to_word) <- word_to_index


decoded_review <- sapply(x_train[1,], function(index) {
  word <- if (index >= 3) index_to_word[[as.character(index - 3)]]
  if (!is.null(word)) word else "?"
})



first_sentiment_result <- y_train[1]

cat(decoded_review)
```

The above movie review is of sentiment:
```{r}
if(first_sentiment_result==1)print("Positive Sentiment") else print("Negative Sentiment")
```


## Simple Recurrent Neural Network (RNN)
Building the RNN model is different from Neural Network. With two changes made to what we have learnt from Artificial Neural Network (ANN) lecture:

**layer_embedding()** :This is used to fit an embedding layer based on the training dataset. Embedding is needed to give the categorical integers (from tokenization) to more meaningful representations. The word embedding captures the inherited relationship of words and dramatically reduces the input dimension (128 dims). This step is needed as tokenization is a naive mapping of words to numbers. With embedding, the converted input would have meaning amongst the numbers.

**layer_simple_rnn()** : This is used to add a simple RNN layer. This layer encapsulates what we have explained earlier namely Recurrence + Neural Networks. 

```{r}
#Build the RNN Model
rnn_model <- keras_model_sequential()
rnn_model %>% 
  layer_embedding(input_dim = max_unique_word, output_dim=128) %>% 
  layer_simple_rnn(units=64, dropout=0.2, recurrent_dropout=0.2) %>% 
  layer_dense(units=1, activation="sigmoid")
```


```{r}
#Compile the RNN Model
rnn_model %>% compile(
  loss="binary_crossentropy",
  optimizer="adam",
  metrics=c("accuracy")
)

```

```{r}
#Fitting the RNN Model
rnn_history <- rnn_model %>%  fit(
  x_train, y_train,
  batch_size= 128,
  epochs=5,
  validation_split=0.2,
  verbose=0
)
```

```{r}
#Evaluate the RNN Model
rnn_model %>% 
  evaluate(x_test, y_test)
```
Our simple RNN model achieved an accuracy of: 71.3%


# How To: LSTM for IMDB Sentiment Analysis
The performance of RNN might not be good in terms of long-term dependencies. This is because of the way it updates the partial derivatives through  Back Propagation Through Time (BPTT), the values would be small for much earlier words compared to the current word.  

Due to this locality disadvantage within RNN, an improvement to this would be to use Long Short Term Memory RNN model (LSTM), which would perform better than RNN by carrying useful information from the earlier words to the later words. 

The only modification to the RNN code would be replacing **layer_simple_rnn()** to **layer_lstm()**

**layer_lstm()** : This is used to add a LSTM layer. This layer solves the locality issue of RNN.

```{r}
#Build the LSTM Model
lstm_model <- keras_model_sequential()

lstm_model %>%
  layer_embedding(input_dim = max_unique_word, output_dim = 128) %>% 
  layer_lstm(units = 64, dropout = 0.2, recurrent_dropout = 0.2) %>% 
  layer_dense(units = 1, activation = 'sigmoid')

```

```{r}
#Compile the LSTM Model
lstm_model %>% compile(
  loss = 'binary_crossentropy',
  optimizer = 'adam',
  metrics = c('accuracy')
)
```

```{r}
#Fitting the LSTM Model
batch_size = 128
epochs = 5
validation_split = 0.2

lstm_history <- lstm_model %>% fit(
  x_train, y_train,
  batch_size = batch_size,
  epochs = epochs,
  validation_split = validation_split,
  verbose=0
)
```

```{r}
#Evaluate the LSTM Model
lstm_model %>% 
   evaluate(x_test, y_test)
```
Here we see that LSTM performs much better than RNN scoring an accuracy of : 83.6%

# Sources 
1. IMDB dataset source: http://ai.stanford.edu/~amaas/data/sentiment/
2. Implementation of Sentiment Analysis with RNN: https://scientistcafe.com/ids/r/ch12rnn#Overview_for_IMDB_Dataset
3. Implementation of Sentiment Analysis with RNN: https://rstudio-pubs-static.s3.amazonaws.com/486604_46a03b6e83204318bb3d13dad87c10e2.html

